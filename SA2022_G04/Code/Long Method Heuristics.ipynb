{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddae3131",
   "metadata": {},
   "source": [
    "# Long Method Heuristics\n",
    "\n",
    "* df - pandas DataFrame containing the whole dataset\n",
    "* df_train - pandas DataFrame containing the training set (approximately 80% of the dataset, selected by stratified sampling)\n",
    "* d_test - pandas DataFrame containing the test set (approximately 20% of the dataset, selected by stratified sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dcb8930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 rows of the training dataframe:\n",
      "    parts  label  sample_id  constructor  cbo  wmc  rfc  loc  returnsQty  \\\n",
      "0  train      0    3698602            0    4    1    0    3           1   \n",
      "1  train      0    3698665            0    5    1    2    4           1   \n",
      "2  train      0    3698860            0    0    1    0    3           1   \n",
      "3  train      0    3699227            0    0    1    0    3           1   \n",
      "4  train      0    3699521            0    1    1    1    8           1   \n",
      "\n",
      "   variablesQty  ...  numbersQty  assignmentsQty  mathOperationsQty  \\\n",
      "0             0  ...           0               0                  0   \n",
      "1             0  ...           0               0                  0   \n",
      "2             0  ...           0               0                  2   \n",
      "3             0  ...           0               0                  0   \n",
      "4             1  ...           0               4                  0   \n",
      "\n",
      "   maxNestedBlocksQty  anonymousClassesQty  lambdasQty  uniqueWordsQty  \\\n",
      "0                   0                    0           0               3   \n",
      "1                   0                    0           0               7   \n",
      "2                   0                    0           0              12   \n",
      "3                   0                    0           0               4   \n",
      "4                   0                    0           0              10   \n",
      "\n",
      "   modifiers  logStatementsQty  hasJavaDoc  \n",
      "0          1                 0           0  \n",
      "1          1                 0           0  \n",
      "2          1                 0           0  \n",
      "3          1                 0           0  \n",
      "4          1                 0           0  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "*****************************************************************\n",
      "The labels of the first 5 lines of the training dataframe:\n",
      " 0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64\n",
      "*****************************************************************\n",
      "The first 5 rows of the testing dataframe:\n",
      "    parts  label  sample_id  constructor  cbo  wmc  rfc  loc  returnsQty  \\\n",
      "8   test      0    3701614            0    3    2    2    9           2   \n",
      "10  test      0    3709545            0    3    1    1    3           1   \n",
      "13  test      0    3719020            1    1    1    0    6           0   \n",
      "15  test      0    3722557            0    2    1    2    5           1   \n",
      "21  test      0    3735879            0    0    1    0    3           0   \n",
      "\n",
      "    variablesQty  ...  numbersQty  assignmentsQty  mathOperationsQty  \\\n",
      "8              0  ...           0               0                  0   \n",
      "10             0  ...           0               0                  0   \n",
      "13             0  ...           0               4                  0   \n",
      "15             0  ...           0               0                  0   \n",
      "21             0  ...           0               1                  0   \n",
      "\n",
      "    maxNestedBlocksQty  anonymousClassesQty  lambdasQty  uniqueWordsQty  \\\n",
      "8                    1                    0           0               5   \n",
      "10                   0                    0           0               4   \n",
      "13                   0                    0           0              12   \n",
      "15                   0                    0           0              10   \n",
      "21                   0                    0           0               5   \n",
      "\n",
      "    modifiers  logStatementsQty  hasJavaDoc  \n",
      "8           1                 0           0  \n",
      "10          9                 0           0  \n",
      "13          2                 0           0  \n",
      "15          1                 0           1  \n",
      "21          1                 0           0  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "*****************************************************************\n",
      "The labels of the first 5 lines of the testing dataframe:\n",
      " 8     0\n",
      "10    0\n",
      "13    0\n",
      "15    0\n",
      "21    0\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "\n",
    "df = pd.read_excel(\"../Data/Long_Method_code_metrics_values.xlsx\")\n",
    "new_labels = {\"label\":     {\"critical\": 1, \"major\": 1, \"minor\": 1, \"none\": 0}}\n",
    "df= df.replace(new_labels)\n",
    "\n",
    "df_train = df[df['parts']=='train']\n",
    "y_train = df_train['label']\n",
    "df_test = df[df['parts']=='test']\n",
    "y_test = df_test['label']\n",
    "\n",
    "print(\"The first 5 rows of the training dataframe:\\n\", df_train.head())\n",
    "print(\"*****************************************************************\")\n",
    "print(\"The labels of the first 5 lines of the training dataframe:\\n\", y_train.head())\n",
    "print(\"*****************************************************************\")\n",
    "print(\"The first 5 rows of the testing dataframe:\\n\", df_test.head())\n",
    "print(\"*****************************************************************\")\n",
    "print(\"The labels of the first 5 lines of the testing dataframe:\\n\", y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29c6714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 2408\n",
      "Number of train examples: 1926 ; positive: 223 ; negative:  1703\n",
      "Number of test examples: 482 ; positive: 54 ; negative:  428\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of examples:\", df.shape[0])\n",
    "print(\"Number of train examples:\", df_train.shape[0], \"; positive:\", df_train[y_train==1].shape[0], \"; negative: \", df_train[y_train==0].shape[0])\n",
    "print(\"Number of test examples:\", df_test.shape[0], \"; positive:\", df_test[y_test==1].shape[0], \"; negative: \", df_test[y_test==0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c9ba4",
   "metadata": {},
   "source": [
    "LM_1 is from Fard et al. (2013): Long Method has many lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc0041af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LM_1(df):   \n",
    "    df['LM_1'] = df['loc'] > 50\n",
    "    df['LM_1'].replace({False:0, True:1},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b610748",
   "metadata": {},
   "source": [
    "LM_2 is from Souza et al. (2017): Long Method is huge, complex and has a high number of nested blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea188ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LM_2(df):\n",
    "    df['LM_2'] = (df['loc'] > 30) & (df['wmc'] > 4) & (df['maxNestedBlocksQty'] > 3)\n",
    "    df['LM_2'].replace({False:0, True:1},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9c096",
   "metadata": {},
   "source": [
    "LM_3 is from Liu et al. (2011): Long Method has many lines of code or is highly complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e240cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LM_3(df):    \n",
    "    df['LM_3'] = (df['loc'] > 50) | (df['wmc'] > 10)\n",
    "    df['LM_3'].replace({False:0, True:1},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577c5de9",
   "metadata": {},
   "source": [
    "ALL – LM_1, LM_2, and LM_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb794ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_heuristics(df):\n",
    "    df['ALL'] = df['LM_1'] & df['LM_2'] & df['LM_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc6639",
   "metadata": {},
   "source": [
    "ANY – LM_1, LM_2, or LM_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc72dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_heuristics(df):\n",
    "    df['ANY'] = df['LM_1'] | df['LM_2'] | df['LM_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d5cee",
   "metadata": {},
   "source": [
    "Precision, recall, and F1-measure of the minority (smell) class (denoted as 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eafbc76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def precision(true, predicted):\n",
    "    tn, fp, fn, tp = confusion_matrix(true, predicted).ravel()\n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def recall(true, predicted):\n",
    "    tn, fp, fn, tp = confusion_matrix(true, predicted).ravel()\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def fmeasure(true, predicted):\n",
    "    p = precision(true, predicted)\n",
    "    r = recall(true, predicted)\n",
    "    return 2*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e170fa",
   "metadata": {},
   "source": [
    "Weighted vote – the model calculates the probability of the code sample suffering from the smell as a weighted vote of the individual classifiers. We use the F1-measure achieved on the training set as the weight and apply the softmax function to normalize weights (so they sum to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e94a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#applies softmax for each sets of scores in input:x.\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def get_weights():\n",
    "    true = df['label']\n",
    "    heuristics = ['LM_1', 'LM_2', 'LM_3']\n",
    "    weights = np.zeros(len(heuristics))\n",
    "    for i in range(len(heuristics)):\n",
    "        predicted = df[heuristics[i]]\n",
    "        weights[i] = fmeasure(true, predicted)\n",
    "    print(\"weights:\", weights)\n",
    "    return weights\n",
    "\n",
    "def weighted_vote(df):\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    \n",
    "    heuristic_fmeasures = get_weights()\n",
    "    weights = softmax(heuristic_fmeasures) \n",
    "    df['Weighted_Vote_probability'] = df['LM_1']*weights[0] + df['LM_2']*weights[1] + df['LM_3']*weights[2]\n",
    "    is_smell = df['Weighted_Vote_probability'] > 0.5\n",
    "    print(\"is smell: \",is_smell)\n",
    "    df['Weighted_Vote'] = 0\n",
    "    df['Weighted_Vote'][is_smell] = 1\n",
    "    print(\"Weighted Vote: \", df['Weighted_Vote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d0d14a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applies all heuristics to the df DataFrame \n",
    "def apply_heuristics(df):\n",
    "    LM_1(df)\n",
    "    LM_2(df)\n",
    "    LM_3(df)    \n",
    "    all_heuristics(df)\n",
    "    any_heuristics(df)\n",
    "    weighted_vote(df)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663e824",
   "metadata": {},
   "source": [
    "Part : \"train\", \"test\" or \"all\" Heuristic : GC_1, GC_2, ..., GC_8, ALL, ANY, Weighted_Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeffe1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_calculation(heuristic, part):\n",
    "    if(part==\"train\"):        \n",
    "        true = df_train['label']\n",
    "        predicted = df_train[heuristic]\n",
    "    elif(part==\"test\"):\n",
    "        true = df_test['label']\n",
    "        predicted = df_test[heuristic]\n",
    "    elif(part==\"all\"):        \n",
    "        true = df['label']\n",
    "        predicted = df[heuristic]\n",
    "        \n",
    "    p = precision(true, predicted)\n",
    "    r = recall(true, predicted)\n",
    "    f = fmeasure(true, predicted)\n",
    "    \n",
    "    return p, r, f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c337b9",
   "metadata": {},
   "source": [
    "Part : \"train\", \"test\" or \"all\"\n",
    "Heuristic : LM_1, LM_2, LM_3, ALL, ANY, Weighted_Vote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30fe848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(heuristic, part):\n",
    "    p, r, f = eval_calculation(heuristic, part)\n",
    "    \n",
    "    print(\"Heuristic\", heuristic, \"on\", part, \":\", \"Precision: %.2f\" % p, \"; Recall: %.2f\" % r, \"; F-measure: %.2f\"% f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a2755f",
   "metadata": {},
   "source": [
    "We show weights here to make sure that our code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b254c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [0.38968481 0.36206897 0.53132832]\n",
      "is smell:  0       False\n",
      "1       False\n",
      "2       False\n",
      "3       False\n",
      "4       False\n",
      "        ...  \n",
      "2403    False\n",
      "2404    False\n",
      "2405    False\n",
      "2406    False\n",
      "2407    False\n",
      "Name: Weighted_Vote_probability, Length: 2408, dtype: bool\n",
      "Weighted Vote:  0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "2403    0\n",
      "2404    0\n",
      "2405    0\n",
      "2406    0\n",
      "2407    0\n",
      "Name: Weighted_Vote, Length: 2408, dtype: int64\n",
      "weights: [0.38968481 0.36206897 0.53132832]\n",
      "is smell:  0       False\n",
      "1       False\n",
      "2       False\n",
      "3       False\n",
      "4       False\n",
      "        ...  \n",
      "2402    False\n",
      "2403    False\n",
      "2404    False\n",
      "2405    False\n",
      "2406    False\n",
      "Name: Weighted_Vote_probability, Length: 1926, dtype: bool\n",
      "Weighted Vote:  0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "2402    0\n",
      "2403    0\n",
      "2404    0\n",
      "2405    0\n",
      "2406    0\n",
      "Name: Weighted_Vote, Length: 1926, dtype: int64\n",
      "weights: [0.38968481 0.36206897 0.53132832]\n",
      "is smell:  8       False\n",
      "10      False\n",
      "13      False\n",
      "15      False\n",
      "21      False\n",
      "        ...  \n",
      "2385    False\n",
      "2387    False\n",
      "2388    False\n",
      "2399    False\n",
      "2407    False\n",
      "Name: Weighted_Vote_probability, Length: 482, dtype: bool\n",
      "Weighted Vote:  8       0\n",
      "10      0\n",
      "13      0\n",
      "15      0\n",
      "21      0\n",
      "       ..\n",
      "2385    0\n",
      "2387    0\n",
      "2388    0\n",
      "2399    0\n",
      "2407    0\n",
      "Name: Weighted_Vote, Length: 482, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "apply_heuristics(df)\n",
    "apply_heuristics(df_train)\n",
    "apply_heuristics(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2755c77",
   "metadata": {},
   "source": [
    "Printing the performance of each heuristic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3281a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic ANY on train : Precision: 0.85 ; Recall: 0.47 ; F-measure: 0.61\n",
      "Heuristic ANY on test : Precision: 0.86 ; Recall: 0.33 ; F-measure: 0.48\n",
      "Heuristic ANY on all : Precision: 0.85 ; Recall: 0.44 ; F-measure: 0.58\n"
     ]
    }
   ],
   "source": [
    "print_result('ANY', 'train')\n",
    "print_result('ANY', 'test')\n",
    "print_result('ANY', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45dd6ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic Weighted_Vote on train : Precision: 0.92 ; Recall: 0.31 ; F-measure: 0.46\n",
      "Heuristic Weighted_Vote on test : Precision: 1.00 ; Recall: 0.26 ; F-measure: 0.41\n",
      "Heuristic Weighted_Vote on all : Precision: 0.93 ; Recall: 0.30 ; F-measure: 0.45\n"
     ]
    }
   ],
   "source": [
    "print_result('Weighted_Vote', 'train')\n",
    "print_result('Weighted_Vote', 'test')\n",
    "print_result('Weighted_Vote', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec3ac948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic ALL on train : Precision: 0.96 ; Recall: 0.11 ; F-measure: 0.20\n",
      "Heuristic ALL on test : Precision: 1.00 ; Recall: 0.11 ; F-measure: 0.20\n",
      "Heuristic ALL on all : Precision: 0.97 ; Recall: 0.11 ; F-measure: 0.20\n"
     ]
    }
   ],
   "source": [
    "print_result('ALL', 'train')\n",
    "print_result('ALL', 'test')\n",
    "print_result('ALL', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef7fce2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic LM_3 on train : Precision: 0.86 ; Recall: 0.40 ; F-measure: 0.55\n",
      "Heuristic LM_3 on test : Precision: 0.89 ; Recall: 0.31 ; F-measure: 0.47\n",
      "Heuristic LM_3 on all : Precision: 0.87 ; Recall: 0.38 ; F-measure: 0.53\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_result('LM_3', 'train')\n",
    "print_result('LM_3', 'test')\n",
    "print_result('LM_3', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cfb2bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic LM_2 on train : Precision: 0.88 ; Recall: 0.24 ; F-measure: 0.37\n",
      "Heuristic LM_2 on test : Precision: 0.91 ; Recall: 0.19 ; F-measure: 0.31\n",
      "Heuristic LM_2 on all : Precision: 0.89 ; Recall: 0.23 ; F-measure: 0.36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_result('LM_2', 'train')\n",
    "print_result('LM_2', 'test')\n",
    "print_result('LM_2', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d33a0b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic LM_1 on train : Precision: 0.93 ; Recall: 0.26 ; F-measure: 0.40\n",
      "Heuristic LM_1 on test : Precision: 1.00 ; Recall: 0.20 ; F-measure: 0.34\n",
      "Heuristic LM_1 on all : Precision: 0.94 ; Recall: 0.25 ; F-measure: 0.39\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_result('LM_1', 'train')\n",
    "print_result('LM_1', 'test')\n",
    "print_result('LM_1', 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038cf012",
   "metadata": {},
   "source": [
    "Result of Table 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fe04d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(index, data):\n",
    "    rule = ['MLOC > 50', 'MLOC > 30 & VG > 4 & NBD > 3', 'MLOC > 50 | VG > 10', 'ALL', 'ANY', 'Weighted Vote']\n",
    "    trainP = []\n",
    "    trainR = []\n",
    "    trainF = []\n",
    "    testP = []\n",
    "    testR = []\n",
    "    testF = []\n",
    "    allP = []\n",
    "    allR = []\n",
    "    allF = []\n",
    "    for i in range(len(index)):\n",
    "        p1, r1, f1 = eval_calculation(index[i], 'train')\n",
    "        trainP.append(round(p1,2))\n",
    "        trainR.append(round(r1,2))\n",
    "        trainF.append(round(f1,2))\n",
    "        \n",
    "        p2, r2, f2 = eval_calculation(index[i], 'test')\n",
    "        testP.append(round(p2,2))\n",
    "        testR.append(round(r2,2))\n",
    "        testF.append(round(f2,2))\n",
    "        \n",
    "        p3, r3, f3 = eval_calculation(index[i], 'all')\n",
    "        allP.append(round(p3,2))\n",
    "        allR.append(round(r3,2))\n",
    "        allF.append(round(f3,2))\n",
    "        \n",
    "        p1 = p2 = p3 = r1 = r2 = r3 = f1 = f2 = f3 = 0\n",
    "        \n",
    "    data['Rule specification'] = rule\n",
    "    data['Training set-P'] = trainP\n",
    "    data['Training set-R'] = trainR\n",
    "    data['Training set-F'] = trainF\n",
    "    data['Test set-P'] = testP\n",
    "    data['Test set-R'] = testR\n",
    "    data['Test set-F'] = testF\n",
    "    data['All-P'] = allP\n",
    "    data['All-R'] = allR\n",
    "    data['All-F'] = allF\n",
    "    \n",
    "    df = pd.DataFrame(data,index)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb1bcb",
   "metadata": {},
   "source": [
    "Show dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d6c7bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rule specification</th>\n",
       "      <th>Training set-P</th>\n",
       "      <th>Training set-R</th>\n",
       "      <th>Training set-F</th>\n",
       "      <th>Test set-P</th>\n",
       "      <th>Test set-R</th>\n",
       "      <th>Test set-F</th>\n",
       "      <th>All-P</th>\n",
       "      <th>All-R</th>\n",
       "      <th>All-F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LM_1</th>\n",
       "      <td>MLOC &gt; 50</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LM_2</th>\n",
       "      <td>MLOC &gt; 30 &amp; VG &gt; 4 &amp; NBD &gt; 3</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LM_3</th>\n",
       "      <td>MLOC &gt; 50 | VG &gt; 10</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALL</th>\n",
       "      <td>ALL</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANY</th>\n",
       "      <td>ANY</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted_Vote</th>\n",
       "      <td>Weighted Vote</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Rule specification  Training set-P  Training set-R  \\\n",
       "LM_1                              MLOC > 50            0.93            0.26   \n",
       "LM_2           MLOC > 30 & VG > 4 & NBD > 3            0.88            0.24   \n",
       "LM_3                    MLOC > 50 | VG > 10            0.86            0.40   \n",
       "ALL                                     ALL            0.96            0.11   \n",
       "ANY                                     ANY            0.85            0.47   \n",
       "Weighted_Vote                 Weighted Vote            0.92            0.31   \n",
       "\n",
       "               Training set-F  Test set-P  Test set-R  Test set-F  All-P  \\\n",
       "LM_1                     0.40        1.00        0.20        0.34   0.94   \n",
       "LM_2                     0.37        0.91        0.19        0.31   0.89   \n",
       "LM_3                     0.55        0.89        0.31        0.47   0.87   \n",
       "ALL                      0.20        1.00        0.11        0.20   0.97   \n",
       "ANY                      0.61        0.86        0.33        0.48   0.85   \n",
       "Weighted_Vote            0.46        1.00        0.26        0.41   0.93   \n",
       "\n",
       "               All-R  All-F  \n",
       "LM_1            0.25   0.39  \n",
       "LM_2            0.23   0.36  \n",
       "LM_3            0.38   0.53  \n",
       "ALL             0.11   0.20  \n",
       "ANY             0.44   0.58  \n",
       "Weighted_Vote   0.30   0.45  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = ['LM_1','LM_2','LM_3','ALL','ANY','Weighted_Vote']\n",
    "data = {'Rule specification':[], 'Training set-P':[], 'Training set-R':[], 'Training set-F':[], \n",
    "        'Test set-P':[], 'Test set-R':[], 'Test set-F':[], \n",
    "        'All-P':[], 'All-R':[], 'All-F':[]}\n",
    "df = result(index, data)\n",
    "df\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc50f34",
   "metadata": {},
   "source": [
    "Change dataframe to csv format and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5be8818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "df.to_csv(os.path.join('../Outputs/Tables','Long_Method_Table_6.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73602b57",
   "metadata": {},
   "source": [
    "Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06cc869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualization():\n",
    "    barWidth = 0.25\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    preci = []\n",
    "    rec = []\n",
    "    fmeas = []\n",
    "    heuristics = []\n",
    "    df2 = df[['Test set-P', 'Test set-R', 'Test set-F']]\n",
    "    df_visual = df2.sort_values('Test set-F')\n",
    "    \n",
    "    preci = df_visual['Test set-P']\n",
    "    rec = df_visual['Test set-R']\n",
    "    fmeas = df_visual['Test set-F']\n",
    "    heuristics = df_visual.index\n",
    "        \n",
    "    br1 = np.arange(len(preci))\n",
    "    br2 = [x + barWidth for x in br1]\n",
    "    br3 = [x + barWidth for x in br2]\n",
    "    \n",
    "    plt.bar(br1, preci, color = 'b', width = barWidth, label = 'Precision')\n",
    "    plt.bar(br2, rec, color = 'g', width = barWidth, label = 'Recall')\n",
    "    plt.bar(br3, fmeas, color = 'orange', width = barWidth, label = 'F-measure')\n",
    "    \n",
    "    plt.xlabel('Long Method Heuristics', fontweight = 'bold', fontsize = 15)\n",
    "    plt.ylabel('Values', fontweight = 'bold', fontsize = 15)\n",
    "    plt.xticks([r + barWidth for r in range(len(preci))],heuristics)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    fig.savefig('../Outputs/Figures/Heuristic_based_approaches_Long_Method_test_set_Fig_5.png', dpi=fig.dpi)\n",
    "    \n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e218c3a1",
   "metadata": {},
   "source": [
    "Fig 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593253e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98fc8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
